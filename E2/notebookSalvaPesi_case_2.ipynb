{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exercise 2**\n",
    "## **Group members**:\n",
    "* Antonio Feltrin, antonio.feltrin@studenti.unipd.it, ID 2097126\n",
    "* Giosuè Sardo Infirri, giosue.sardoinfirri@studenti.unipd.it, ID 2090564 \n",
    "* Riccardo Tancredi, riccardo.tancredi@studenti.unipd.it, ID 2089395\n",
    "* Simone Toso, simone.toso.2@studenti.unipd.it, ID 2095484"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Import all necessary dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "plt.rcParams['font.size'] = 14\n",
    "from IPython.display import display_html \n",
    "import pandas as pd\n",
    "import gzip\n",
    "import matplotlib as mplt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Flow control**: control and process of the data we generate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type of encoding\n",
    "one_hot = True\n",
    "\n",
    "# sigmoid takes into account energy difference =2\n",
    "SPINS = False\n",
    "# type of gradient: Adam or Vanilla SGD\n",
    "ADAM = True\n",
    "\n",
    "# centering trick\n",
    "centering = False\n",
    "# Data: we start with data_b (CASE = 1 in the lecture notebook) and then later we will study data_c \n",
    "data = \"c\" \n",
    "# CD-n iterations\n",
    "CD = 2\n",
    "# mini batch size for the SGD in the RBM training\n",
    "mini_batch_size = 36\n",
    "# Number of epochs for the RBM training\n",
    "Number_of_epochs = 200\n",
    "\n",
    "# Draw weights function of the hidden unit in a A*G table\n",
    "draw_weigths = True\n",
    "\n",
    "# If we want to save data\n",
    "save_file = True\n",
    "\n",
    "#If we want to save weights\n",
    "save_weights = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data generation**: we generate the data as the CASE = 1 shown in class. Data from the folder \"Data_c\", where there is the file with the unknown data, are studied at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname='DATA_'+data+'/x_RBM_q0.2.dat'\n",
    "# loading data: each row is a list of visible units\n",
    "# NOTE: data \"x\" here is named \"v\" for \"visible\"\n",
    "x = np.loadtxt(fname, delimiter=\" \",dtype=int)\n",
    "\n",
    "\n",
    "# random seed for reproducibility\n",
    "np.random.seed(2090564)\n",
    "\n",
    "# size of dataset\n",
    "N = 10000\n",
    "# number of block for one-hot encoding\n",
    "G = 5\n",
    "# number of categories\n",
    "A = int(6)\n",
    "# number of categories of kind 0\n",
    "A0 = int(A/2)\n",
    "# number of categories of kind 1\n",
    "A1 = A-A0\n",
    "#\n",
    "AA = [A0, A1]\n",
    "# size of each data point\n",
    "L = G*A\n",
    "#  \n",
    "# Noise: probability q to change category\n",
    "q = 0.1  # 0.1 then \n",
    "#\n",
    "# a = 0.6\n",
    "print(f\"L={L}, A={A}, G={G}\")\n",
    "\n",
    "# x,y = np.zeros((N,L)).astype(int), np.zeros(N).astype(int)\n",
    "\n",
    "# if data == \"b\":\n",
    "#     for n in range(N):\n",
    "#         # kind of the first slot\n",
    "#         k = np.random.randint(0,2)\n",
    "#         # label\n",
    "#         y[n] = k\n",
    "#         i0=0\n",
    "#         for g in range(G):\n",
    "#             # normal choice\n",
    "#             i = np.random.randint(0, AA[k])\n",
    "#             j = i + i0 + k*A0\n",
    "#             if np.random.random()<q:\n",
    "#                 # normal choice\n",
    "#                 i = np.random.randint(0,A)\n",
    "#                 j = i + i0\n",
    "            \n",
    "#             if(n<2): \n",
    "#                 print(f\"k={k}, g={g}, i0={i0}, i={i}, j={j}, AA[k]={AA[k]}\")\n",
    "#             # one-hot encoding\n",
    "#             x[n][j] = 1\n",
    "#             i0= i0 + A\n",
    "#             k = 1 - k\n",
    "                    \n",
    "#         if(n<3): print(x[n], y[n])\n",
    "\n",
    "#     print(\"...\")    \n",
    "\n",
    "# elif data == \"c\":\n",
    "#     x = np.loadtxt('Data_c/x_RBM_q0.2.dat', delimiter=\" \",dtype=int)\n",
    "\n",
    "# else:\n",
    "#     print(\"Select your data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data print taken from lecture notebooks\n",
    "def is_one(cell_value):\n",
    "    color0 = 'background-color: blue; color: white'\n",
    "    color1 = 'background-color: cyan;'\n",
    "    if type(cell_value) in [float, int]:\n",
    "        if cell_value == 1:\n",
    "            return color1\n",
    "    return color0\n",
    "\n",
    "N1=12\n",
    "df0 = pd.DataFrame(x[:N1])\n",
    "\n",
    "df0s = df0.style.set_table_attributes(\"style='display:inline'\")\n",
    "df0s.applymap(is_one)\n",
    "sty = [dict(selector=\"caption\",props=[(\"font-size\", \"150%\")])]\n",
    "df0s.set_caption('Original').set_table_styles(sty)\n",
    "\n",
    "display_html(df0s._repr_html_(), raw=True)\n",
    "\n",
    "df=df0\n",
    "t=[i for i in range(N1)]\n",
    "g = pd.Categorical(df[t]).codes # convert groups to indexs  \n",
    "df.style.apply(\n",
    "    lambda x: ['background-color: {}'.format(\"#FFCCCC\") for i in g], \n",
    "    subset=[0,1,2,3, 8,9,10,11, 16,17,18,19 ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We work with 6 hidden units (**M = 6**) for both *data_b* and *data_c*. ACtually, when *one_hot = True*, so when we force the RBM to work with the one hot encoding, only **one** hidden unit is sufficient for *data_b* and **two** for *data_c*; we will test this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As seen in class, we name \"v\" for \"visible\" instead of using x\n",
    "v = np.copy(x)\n",
    "# store in v0, because later we will shuffle v\n",
    "v0 = np.copy(v)\n",
    "\n",
    "if SPINS:\n",
    "    GAP = 2 # is given by setting Energy levels = +- 1 and from the Boltzmann distribution getting a sigmoid function with coefficient 2 at the exponent\n",
    "    v = 2*v - 1\n",
    "    vmin = -1\n",
    "else:\n",
    "    GAP = 1\n",
    "    vmin = 0\n",
    "\n",
    "# RBM, number of hidden units\n",
    "M = 2\n",
    "\n",
    "# range of each initial weight, as seen in class\n",
    "sigma = np.sqrt(4. / float(L + M))\n",
    "# random seed for reproducibility\n",
    "np.random.seed(2090564)\n",
    "# initial weights from a Normal distr. (see literature, e.g. page 98 of Mehta's review)\n",
    "w = sigma * np.random.randn(L, M)\n",
    "a = sigma * np.random.randn(L)\n",
    "b = np.zeros(M)\n",
    "print(\"w[:3] =\", w[:3])\n",
    "print(\"a =\", a[:3])\n",
    "print('...')\n",
    "w0, a0, b0 = np.copy(w), np.copy(a), np.copy(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot functions, as done in class\n",
    "def create_coord(np,x0,f=1.0):\n",
    "    x=[x0] * np\n",
    "    print(x)\n",
    "    y=list(range(np))\n",
    "    for i in range(np):\n",
    "        y[i] = f*(y[i]/(3*np-1) - 0.5) if np == 1 else f*(y[i]/(np-1.) - 0.5)\n",
    "    return (x,y)\n",
    "(x1,y1)=create_coord(L,0)\n",
    "(x2,y2)=create_coord(M,1,f=0.7)\n",
    "\n",
    "def mycolor(val):\n",
    "    if val>0: return '#ab2a24'\n",
    "    elif val<0: return 'royalblue'\n",
    "    else: return 'black'\n",
    "    \n",
    "def plotgraph(epoch=0):\n",
    "    fig, ax = plt.subplots(1,1 , figsize=(7, 4))\n",
    "    ax.tick_params(left=False,bottom=False)\n",
    "    ax.xaxis.set_major_formatter(NullFormatter())\n",
    "    ax.yaxis.set_major_formatter(NullFormatter())\n",
    "    \n",
    "    A=1./max(w.max(),-w.min())\n",
    "    for i in range(L):\n",
    "        for j in range(M):\n",
    "            ex, ey, col = (y1[i],y2[j]),(x1[i],x2[j]),mycolor(w[i][j])\n",
    "            ax.plot(ex, ey, col, zorder=1, alpha=A*abs(w[i][j]))\n",
    "    # Scatter plot on top of lines\n",
    "    #A=300./(a.max()+b.max())\n",
    "    A=500.\n",
    "    for i in range(L):\n",
    "        ax.scatter(y1[i],x1[i], s=A*abs(a[i]), zorder=2, c=mycolor(a[i]))\n",
    "    for j in range(M):\n",
    "        ax.scatter(y2[j], x2[j], s=min(300,A*abs(b[j])), zorder=2, c=mycolor(b[j]), marker=\"s\")\n",
    "    ax.set_title(f'>0 red, <0 blue, epoch={epoch}')\n",
    "    ax.text(-0.5,0.9,\"hidden\\nlayer\")\n",
    "    plt.show()\n",
    "                  \n",
    "plotgraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eq(213) page 97, activation via sigmoid\n",
    "# Taking into account energy gap DE = 2 for \"spin\" variables (-1,1)\n",
    "def activate(v_in, wei, bias, DE, info=False): \n",
    "    act = np.dot(v_in, wei) + bias\n",
    "    n = np.shape(act)\n",
    "    prob = 1. / (1. + np.exp(-DE*act))              # updated sigmoid function\n",
    "    v_out = np.full(n, vmin, dtype=int)             # a list on -1's or 0's\n",
    "    v_out[np.random.random_sample(n) < prob] = 1    # activate the 1's with probability prob\n",
    "    if info:\n",
    "        print('input=', v_in)\n",
    "        print('act=',act)\n",
    "        print('prob=',prob)\n",
    "        print('output=',v_out)\n",
    "    return v_out\n",
    "\n",
    "k = 0\n",
    "# activate(v[k],w,b,GAP,info=True)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We use the method of fixing the one-hot encoding. We remind that, given the possibilities $[1,0,0,0], [0,1,0,0], [0,0,1,0], [0,0,0,1]$, the energy of each group is given by $$-E = \\sum_i a_i v_i + \\sum_{i,\\mu}W_{i\\mu}v_ih_\\mu$$ In theory there would also be the term $\\sum_\\mu b_\\mu h_\\mu$. However, since this term is the same for all of the possible encodings, we will neglect it when evaluating the Boltzmann weight of each encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_generator(nBlocks, w, h, a, ampl=1): # nBlocks = G\n",
    "    encodings = [np.array([0,0,0,0,0,1]), np.array([0,0,0,0,1,0]), np.array([0,0,0,1,0,0]), np.array([0,0,1,0,0,0]), np.array([0,1,0,0,0,0]), np.array([1,0,0,0,0,0])]\n",
    "    if SPINS:\n",
    "        for i in range(len(encodings)):\n",
    "            encodings[i] = 2*encodings[i] - np.ones(A, dtype=int)       # change of representation if Spins == True, so when GAP = 2 and vmin = -1\n",
    "    vf = np.zeros(L)    # new configuration initialized as series of zeros\n",
    "    for block in range(nBlocks):    \n",
    "        # for each block of 4 bits we try all encodings and choose one at random with probability given by the boltzmann distribution\n",
    "        probabilities = []\n",
    "        for encoding in encodings: # for each possible encoding 1000, 0100, ... we evaluate the energy and its corresponding probability\n",
    "            energy = np.dot(a[block*A: (block+1)*A] + w[block*A:(block+1)*A, ]@h, encoding) # interaction of the chosen block with its local field\n",
    "            probabilities.append(np.exp(energy*ampl))\n",
    "        probabilities = probabilities / sum(probabilities)          # normalized probability    \n",
    "        cumulatives = [sum(probabilities[:i+1]) for i in range(A)]  # cumulative probability function \n",
    "        random = np.random.uniform(0., 1.)      \n",
    "        # Here we look for the first index at which the cumulatives are higher than the uniform random variable\n",
    "        choice = A-1\n",
    "        for i in range(A):\n",
    "            if cumulatives[i] > random : \n",
    "                choice = i\n",
    "                break \n",
    "        # Now choice is the index of the encoding we wanted\n",
    "        vf[block*A:(block+1)*A] = encodings[choice]\n",
    "    return vf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ADAM class**: implementation of Adam algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from https://towardsdatascience.com/how-to-implement-an-adam-optimizer-from-scratch-76e7b217f1cc\n",
    "class AdamOptim():\n",
    "    def __init__(self, eta=1e-3, beta1=0.9, beta2=0.99, epsilon=1e-8):\n",
    "        ## Initialization of Adam variables and coefficients\n",
    "        self.m_dw, self.s_dw = np.zeros((L, M)), np.zeros((L, M))\n",
    "        self.m_da, self.s_da = np.zeros(L), np.zeros(L)\n",
    "        self.m_db, self.s_db = np.zeros(M), np.zeros(M)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.eta = eta\n",
    "        \n",
    "    def update(self, t, w, a, b, dw, da, db):\n",
    "        ## dw, da, db are from current minibatch\n",
    "        \n",
    "        ### first momenta beta 1\n",
    "        # *** weights *** #\n",
    "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
    "        # *** biases of hidden units *** #        \n",
    "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
    "        # *** biases *** #\n",
    "        self.m_da = self.beta1*self.m_da + (1-self.beta1)*da\n",
    "\n",
    "        ### second momenta beta 2\n",
    "        # *** weights *** #\n",
    "        self.s_dw = self.beta2*self.s_dw + (1-self.beta2)*(dw**2)\n",
    "        # *** biases of hidden units *** #\n",
    "        self.s_da = self.beta2*self.s_da + (1-self.beta2)*(da**2)\n",
    "        # *** biases *** #\n",
    "        self.s_db = self.beta2*self.s_db + (1-self.beta2)*(db**2)\n",
    "\n",
    "        ## bias correction via first and second momenta\n",
    "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
    "        m_da_corr = self.m_da/(1-self.beta1**t)\n",
    "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
    "        s_dw_corr = self.s_dw/(1-self.beta2**t)\n",
    "        s_da_corr = self.s_da/(1-self.beta2**t)\n",
    "        s_db_corr = self.s_db/(1-self.beta2**t)\n",
    "\n",
    "        ## update weights and biases\n",
    "        w = w + self.eta*(m_dw_corr/(np.sqrt(s_dw_corr)+self.epsilon))\n",
    "        a = a + self.eta*(m_da_corr/(np.sqrt(s_da_corr)+self.epsilon))\n",
    "        b = b + self.eta*(m_db_corr/(np.sqrt(s_db_corr)+self.epsilon))\n",
    "        \n",
    "        ## return updated weights and biases\n",
    "        return w, a, b "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic steps of vanilla gradient descent, from eq.(211)\n",
    "def vanilla(w, a, b, dw, da, db):\n",
    "    w = w + dw\n",
    "    a = a + da\n",
    "    b = b + db\n",
    "    return w, a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Log-likelihood**: log-likelihood as a function of the epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_is(v, h, w, a, b):\n",
    "    # computation of the current energy\n",
    "    eng = - np.dot(a,v) - np.dot(b,h) - np.dot(v.T@w,h)\n",
    "    return eng\n",
    "\n",
    "# In order to compute the Z partition function, one_hot_generator2 takes in input a string in base 4 (e.g. 0123) \n",
    "# and outputs the corresponding configuration of blocks with one hot encoding (e.g. 0001 0010 0100 1000) \n",
    "def one_hot_generator2(input, A, G):\n",
    "    # encodings = [np.array([0,0,0,1]), np.array([0,0,1,0]), np.array([0,1,0,0]), np.array([1,0,0,0])]\n",
    "    encodings = [np.array([0,0,0,0,0,1]), np.array([0,0,0,0,1,0]), np.array([0,0,0,1,0,0]), np.array([0,0,1,0,0,0]), np.array([0,1,0,0,0,0]), np.array([1,0,0,0,0,0])]\n",
    "    output = []\n",
    "    for i in input:\n",
    "        output.append(encodings[int(i)])\n",
    "    output = np.array(output)\n",
    "    output = output.reshape(A*G,)\n",
    "    if SPINS:\n",
    "        # change of representation: here vmin = -1\n",
    "        output = output*GAP + np.full(A*G,vmin) \n",
    "    return output\n",
    "\n",
    "# This function computes the Z partition function and the log-likelihood\n",
    "def log_likelihood(w, a, b, v, h):\n",
    "    eng, Z= 0, 0 \n",
    "    for i in range(N):\n",
    "        # we compute the energy for each data point {v, h}, where h is the set of hidden variables generated\n",
    "        eng += energy_is(v[i], h[i], w, a, b)\n",
    "    eng /= v.shape[0] # v.shape[0] = N: average among all the possibilities\n",
    "\n",
    "    # total number of configurations = (A**G)*(2**M)\n",
    "    for i in range(int(A**G)): # A**G = 4**5 1024, 2*M\n",
    "        vgen = np.base_repr(i, base=A).zfill(G)     # base 4 representation of the visible units: this is the input of the one_hot_generator2 function\n",
    "        vgen = one_hot_generator2(vgen, A, G)\n",
    "        for j in range(int(2**M)):\n",
    "            hgen = np.base_repr(j, base=2).zfill(M) # base 2 representation of the hidden units\n",
    "            hgen = np.array([int(x) for x in str(hgen)])\n",
    "            if SPINS:\n",
    "                # change of representation: here vmin = -1\n",
    "                hgen = hgen*GAP + np.full(M, vmin)\n",
    "            # The partition function is the sum of all the Boltzmann factors\n",
    "            Z += np.exp(-energy_is(vgen, hgen, w, a, b))\n",
    "\n",
    "    # Log-likelihood term\n",
    "    L = - eng - np.log(Z) # L is going to be maximized    \n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Adversarial Accuracy Indicator**: other quality indicator described in the paper by Decelle et al (pag. 3 supplementive material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_AAI(v, h):\n",
    "    v_copy = np.copy(v)\n",
    "    h_copy = np.copy(h)\n",
    "    if SPINS:\n",
    "        # change of representation: here vmin = -1\n",
    "        v_copy = ((v_copy + 1)/2).astype(int)\n",
    "        h_copy = ((h_copy + 1)/2).astype(int)\n",
    "    # Calculation of A_S & A_T\n",
    "    A_S, A_T = 0, 0\n",
    "    \n",
    "    for i in range(v.shape[0]):\n",
    "        # Bitwise operation to spot all the different elements\n",
    "        # The sum all the errors represents the distance searched function: the more elements are different the higher the distance function will be \n",
    "        distances_TS = np.sum((h_copy[i] ^ v_copy), axis=1)\n",
    "        distances_ST = np.sum((v_copy[i] ^ h_copy), axis=1)\n",
    "        distances_SS = np.sum((v_copy[i] ^ v_copy), axis=1)\n",
    "        distances_TT = np.sum((h_copy[i] ^ h_copy), axis=1)\n",
    "        \n",
    "        # Update of the diagonal element: in this way the diagonal elements are no more zero\n",
    "        # We sum to the diagonal element +30 (+20 it's still ok) so that the diagonal elements are not spotted in the next computation\n",
    "        # This is done because there could be non diagonal elements for which the distance is zero.\n",
    "        distances_SS[i] = 30\n",
    "        distances_TT[i] = 30\n",
    "\n",
    "        if np.min(distances_SS) < np.min(distances_ST):\n",
    "            A_S += 1\n",
    "        elif np.min(distances_SS) == np.min(distances_ST) and one_hot:\n",
    "            A_S += 0.5\n",
    "        else:\n",
    "            A_S += 0\n",
    "        \n",
    "        if np.min(distances_TT) < np.min(distances_TS):\n",
    "            A_T += 1\n",
    "        elif np.min(distances_TT) == np.min(distances_TS) and one_hot:\n",
    "            A_T += 0.5\n",
    "        else:\n",
    "            A_T += 0\n",
    "\n",
    "    print(\"Calculating the adversarial accuracy indicator...\")\n",
    "    return A_S/v.shape[0], A_T/v.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the MSE estimate (energy) of the two indicators (pag. 3 supplementive material)\n",
    "def E(v, v1):\n",
    "    A_S, A_T = E_AAI(v, v1)\n",
    "    return A_S, A_T, (A_S-0.5)**2+(A_T-0.5)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Our score**: we wanted to check the fraction of data that follows the pattern (*left-right* polarization) that we'd defined when generating the dataset. This will be useful in analyzing the goodness of the final denoised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_score(v1):\n",
    "    scores = 0 \n",
    "    \n",
    "    dfscore = pd.DataFrame(((v1[:N]-vmin)/(1-vmin)).astype(int))\n",
    "\n",
    "    nBlocks = int(L/4)\n",
    "\n",
    "    for nRow in range(N): \n",
    "        # We check the alternance of left-right polarization\n",
    "        row = dfscore.iloc[nRow,:]\n",
    "        positions = [] \n",
    "        corretto = True\n",
    "        for block in range(nBlocks): # DSDSDSDS --> check that this configuration alternates left-right\n",
    "            doub = 0\n",
    "            blocchetto = row[block*4:(block+1)*4]\n",
    "            index = 0\n",
    "            for i in range(4):\n",
    "                if blocchetto[block*4 + i] == 1:\n",
    "                    index = i\n",
    "                    doub += 1\n",
    "            if doub != 1:\n",
    "                corretto = False\n",
    "            if index < 2:\n",
    "                positions.append(0) # 0 --> left\n",
    "                \n",
    "            else:\n",
    "                positions.append(1) # 1 --> right\n",
    "    \n",
    "        for index in range(len(positions)-1):           # for each 0 1 in the array\n",
    "            if positions[index] == positions[index+1]:  # check if there is repetition \n",
    "                corretto = False\n",
    "        if corretto == True:\n",
    "            scores += 1             # update the score result\n",
    "\n",
    "    # return the normalized score value\n",
    "    return  scores/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Entropy**: another indicator shown in the paper by Decelle et al (pag. 3-4 supplementive material)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def S(v, v1):\n",
    "    idx1 = np.random.randint(v.shape[0], size=int(v.shape[0]/2))\n",
    "    idx2 = np.random.randint(v1.shape[0], size=int(v1.shape[0]/2))\n",
    "    v_1_new = np.concatenate((v[idx1, :], v1[idx2, :]))\n",
    "    delta_entropy = len(gzip.compress(v))/len(gzip.compress(v_1_new)) -1 \n",
    "    return delta_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RBM train**: we train the model using the parameters set in the initial notebook cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed for reproducibility\n",
    "np.random.seed(2090564)\n",
    "\n",
    "# initial plot\n",
    "plotgraph(0)\n",
    "\n",
    "# learning rate for SGD - vanilla\n",
    "l_rate = 1.0\n",
    "\n",
    "# One-hot encoding\n",
    "nBlocks = int(G)\n",
    "\n",
    "# minibatch\n",
    "mini, m = mini_batch_size, 0\n",
    "\n",
    "# Adam definition\n",
    "adam = AdamOptim()\n",
    "\n",
    "# CD-n iterations\n",
    "n = CD\n",
    "\n",
    "# Time for Adam\n",
    "t = 1 \n",
    "\n",
    "#rate, for calculating stuff\n",
    "printrate = 2\n",
    "\n",
    "# Number of epochs\n",
    "epochs = Number_of_epochs\n",
    "\n",
    "# Centering trick parameters\n",
    "zed_a, zed_b = 0.01, 0.01\n",
    "\n",
    "# energy & entropy sampling\n",
    "A_S, A_T, energy, entropy = [], [], [], []\n",
    "score, log_res = [], []\n",
    "\n",
    "\n",
    "# train model\n",
    "print('===================================================')\n",
    "for epoch in range(epochs):\n",
    "    mu_c, lambda_c= np.full(L, 0.25), np.full(M, 0.25) # μ, λ in centering\n",
    "    # aggregate normalization of batch statistics and learning rate\n",
    "    t = 1 # Adam time reset at each epoch  \n",
    "    for k in range(N):\n",
    "        if m==0:\n",
    "            # initialize averages in miniblock\n",
    "            v_data, v_model = np.zeros(L), np.zeros(L)\n",
    "            h_data, h_model = np.zeros(M), np.zeros(M)\n",
    "            vh_data,vh_model= np.zeros((L,M)), np.zeros((L,M))\n",
    "            if centering:\n",
    "                vdat_c, vmod_c, hdat_c, hmod_c = np.zeros((mini, L)), np.zeros((mini, L)), np.zeros((mini, M)), np.zeros((mini, M))\n",
    "        \n",
    "        # positive CD phase: generating h \n",
    "        h = activate(v[k], w, b, GAP)  \n",
    "        h0 = np.copy(h)\n",
    "        # negative CD phase: generating fantasy vf\n",
    "        for _ in range(n):\n",
    "            # negative CD phase: generating fantasy vf\n",
    "            vf = one_hot_generator(nBlocks, w, h, a) if one_hot else activate(h, w.T, a, GAP)  # fantasy unit\n",
    "            # another positive CD phase: generating fantasy h from fantasy vf \n",
    "            hf = activate(vf, w, b, GAP)\n",
    "            h = np.copy(hf) \n",
    "\n",
    "        # update\n",
    "        v_data  += v[k]\n",
    "        v_model += vf\n",
    "        h_data  += h0   # h\n",
    "        h_model += hf\n",
    "\n",
    "        if not centering:\n",
    "            vh_data += np.outer(v[k].T,h0)  # h\n",
    "            vh_model+= np.outer(vf.T,hf)\n",
    "    \n",
    "        # centering trick\n",
    "        if centering:\n",
    "            vdat_c[m] = v[k]\n",
    "            vmod_c[m] = vf\n",
    "            hdat_c[m] = h0\n",
    "            hmod_c[m] = hf\n",
    "    \n",
    "        # minibatch update\n",
    "        m += 1\n",
    "\n",
    "        if m==mini:\n",
    "\n",
    "            if centering:\n",
    "                # Centering Trick\n",
    "                lambda_batch = h_data/mini\n",
    "                mu_batch = v_data/mini\n",
    "                # Bortoletto uses hi*wij*vj so the opposite of our indexing\n",
    "                a = a + zed_b*np.dot(w, (lambda_batch - lambda_c))\n",
    "                b = b + zed_a*np.dot(w.T, (mu_batch - mu_c))\n",
    "\n",
    "                mu_c = (1 - zed_a)*mu_c + zed_a*mu_batch\n",
    "                lambda_c = (1 - zed_b)*lambda_c + zed_b*lambda_batch\n",
    "\n",
    "                for l in range(mini):\n",
    "                    vh_data += np.outer((vdat_c[l] - mu_c),(hdat_c[l] - lambda_c).T)\n",
    "                    vh_model += np.outer((vmod_c[l] - mu_c),(hmod_c[l] - lambda_c).T)\n",
    "\n",
    "            # gradient of the likelihood: follow it along its positive direction\n",
    "            # Adam or Vanilla update:\n",
    "            dw = (vh_data - vh_model)/mini  # gradient variable\n",
    "            da = (v_data - v_model)/mini    # gradient variable\n",
    "            db = (h_data - h_model)/mini    # gradient variable\n",
    "            w, a, b = adam.update(t, w, a, b, dw, da, db) if ADAM else vanilla(w, a, b, dw*l_rate, da*l_rate, db*l_rate)\n",
    "            \n",
    "            if not ADAM:\n",
    "                l_rate = l_rate / (0.05 * l_rate + 1) \n",
    "            \n",
    "            m = 0\n",
    "            t += 1\n",
    "\n",
    "    if epoch%printrate==(printrate-1):\n",
    "        # plotgraph(epoch+1)\n",
    "\n",
    "        v1 = np.zeros_like(v0)\n",
    "        hfin = np.zeros((N, M))\n",
    "        for k in range(N):\n",
    "            # positive CD phase: generating h \n",
    "            hfin[k] = activate(v0[k],w,b,GAP)  \n",
    "            # negative CD phase: generating fantasy vf\n",
    "            v1[k] = one_hot_generator(nBlocks, w, hfin[k], a) if one_hot else activate(hfin[k],w.T,a,40*GAP)\n",
    "        \n",
    "        # Energy update\n",
    "        a1, a2, en = E(v0, v1)\n",
    "        A_S.append(a1)\n",
    "        A_T.append(a2)\n",
    "        energy.append(en)\n",
    "\n",
    "        # Entropy update\n",
    "        entropy.append(S(v0, v1))\n",
    "\n",
    "        # Our_score update\n",
    "        # sco_temp = our_score(v1)\n",
    "        # score.append(sco_temp)\n",
    "    \n",
    "        # Log-likelihood update\n",
    "        log_temp = log_likelihood(w, a, b, v0, hfin)\n",
    "        log_res.append(log_temp)\n",
    "\n",
    "        print(f\"logl = {log_temp}\")\n",
    "        # print(f\"logl = {log_temp}, score = {sco_temp}\")\n",
    "\n",
    "    # randomize the order of input data\n",
    "    np.random.shuffle(v)\n",
    "    \n",
    "    print('epoch =', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w[:3], a, b, dw[:3], da, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Denoising**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As done in class, until now beta was 1; we put it now equal to 40, so very low temperature, to clean up points\n",
    "ampl = 40.\n",
    "# original, non-reshuffled data in v0\n",
    "v1 = np.zeros_like(v0)\n",
    "for k in range(N):\n",
    "    # positive CD phase: generating h \n",
    "    h = activate(v0[k],w,b,ampl*GAP)\n",
    "    # negative CD phase: generating fantasy vf with low T == large GAP\n",
    "    v1[k] = one_hot_generator(nBlocks, w, h, a, ampl=ampl) if one_hot else activate(h,w.T,a,ampl*GAP)\n",
    "\n",
    "def is_one(cell_value):\n",
    "    color0 = 'background-color: gray; color: white'\n",
    "    color1 = 'background-color: gold;'\n",
    "    if type(cell_value) in [float, int]:\n",
    "        if cell_value == 1:\n",
    "            return color1\n",
    "    return color0\n",
    "\n",
    "df0 = pd.DataFrame(v0[:N1])\n",
    "df1 = pd.DataFrame(((v1[:N1]-vmin)/(1-vmin)).astype(int))\n",
    "\n",
    "df0s = df0.style.set_table_attributes(\"style='display:inline'\")\n",
    "df1s = df1.style.set_table_attributes(\"style='display:inline'\")\n",
    "df0s.applymap(is_one)\n",
    "df1s.applymap(is_one)\n",
    "sty = [dict(selector=\"caption\",props=[(\"font-size\", \"150%\")])]\n",
    "df0s.set_caption('Original').set_table_styles(sty)\n",
    "df1s.set_caption('Denoised').set_table_styles(sty)\n",
    "\n",
    "display_html(df0s._repr_html_()+df1s._repr_html_(), raw=True)\n",
    "\n",
    "t=[i for i in range(N1)]\n",
    "g = pd.Categorical(df[t]).codes # convert groups to indexs  \n",
    "df1.style.apply(\n",
    "    lambda x: ['background-color: {}'.format(\"blue\") for _ in g], \n",
    "    subset=[0,1,2,3, 8,9,10,11, 16,17,18,19 ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We compute our final score after having denoised the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = our_score(v1)\n",
    "print(f\"Final score, after denoising - amplitude = {ampl} - is: {final_score}\")\n",
    "final_AAI = E(v0, v1)\n",
    "print(f\"Final AAI values {final_AAI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final plots of the current simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(0, epochs, printrate)), energy, label=\"energy\", ls=\"--\", marker=\"o\")\n",
    "plt.xlabel(\"# epochs\")\n",
    "plt.ylabel(\"Energy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(0, epochs, printrate)), A_S, label=\"A_S\", ls=\"--\", marker=\"o\")\n",
    "plt.plot(list(range(0, epochs, printrate)), A_T, label=\"A_T\", ls=\"--\", marker=\"o\")\n",
    "plt.xlabel(\"# epochs\")\n",
    "plt.ylabel(\"A\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(0, epochs, printrate)), entropy, label=\"entropy\", ls=\"--\", marker=\"o\")\n",
    "plt.xlabel(\"# epochs\")\n",
    "plt.ylabel(\"Entropy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(list(range(0, epochs, printrate)), score, label=\"our score\", ls=\"--\", marker=\"o\")\n",
    "# plt.xlabel(\"# epochs\")\n",
    "# plt.ylabel(\"Our score\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(0, epochs, printrate)), log_res, label=\"log likelihood\", ls=\"--\", marker=\"o\")\n",
    "plt.xlabel(\"# epochs\")\n",
    "plt.ylabel(\"log likelihood\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"epochs\", \"energy\", \"A_S\", \"A_T\", \"score\", \"entropy\", \"log_likelihood\"])\n",
    "df[\"epochs\"] = list(range(0, epochs, printrate))\n",
    "df[\"energy\"] = energy\n",
    "df[\"A_S\"] = A_S\n",
    "df[\"A_T\"] = A_T\n",
    "# df[\"score\"] = score\n",
    "df[\"entropy\"] = entropy\n",
    "df[\"log_likelihood\"] = log_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Heatmap plot of the weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions taken from https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "def heatmap(data, row_labels, col_labels, ax=None,\n",
    "            cbar_kw=None, cbarlabel=\"\", **kwargs):\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    if cbar_kw is None:\n",
    "        cbar_kw = {}\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, **kwargs) #im now contains the data\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw) \n",
    "    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    ax.tick_params(top=True, bottom=False,\n",
    "                   labeltop=True, labelbottom=False)\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1]+1)-.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0]+1)-.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle='-', linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    return im, cbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We draw tables of the weights to visualize the patterns figured out by the RBM during the training \n",
    "def drawTables(w, nrows, ncols): # takes as input the weights\n",
    "    fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (14,14))\n",
    "    nBlocks = 5 # number of blocks\n",
    "    for block in range(nBlocks):\n",
    "        im, cbar = heatmap(w[block*A:(block+1)*A], np.arange(A)+1, np.arange(M)+1, ax = ax[block//2, block%2], cmap = \"RdBu_r\", cbarlabel = \"weight\", norm = mplt.colors.CenteredNorm())       \n",
    "        ax[block//2, block%2].title.set_text(\"Block \" + str(block+1))\n",
    "    \n",
    "    for axis in ax.flat[nBlocks:]:\n",
    "        axis.remove()\n",
    "    plt.show\n",
    "    \n",
    "if draw_weigths:\n",
    "    drawTables(w,3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p \"My_data_results_6_neuroni\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_file:\n",
    "    dir_name = \"My_data_results_6_neuroni\"\n",
    "    name = f\"Adam_{n}\" if ADAM else f\"Vanilla_{n}\"\n",
    "    name += \"_one_hot\" if one_hot else \"\"\n",
    "    name += \"_CenteringTrick\" if centering else \"\" \n",
    "    name += \"_Spin\" if SPINS else \"_NO-Spin\" \n",
    "    file_name = dir_name+\"/\"+name+\".dat\"\n",
    "    np.savetxt(file_name, df, delimiter=\" \", header=\"epochs, energy, A_S, A_T, score, entropy, log_likelihood\", comments=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_file:\n",
    "    with open(dir_name+\"/\"+\"final_considerations.txt\", \"a\") as f:\n",
    "        f.write(name)\n",
    "        f.write(\" --> our final score:\")\n",
    "        f.write(str(final_score))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\" --> AAI final score (in order A_S, A_T, energy): \")\n",
    "        f.write(str(final_AAI))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{file_name} created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see that everything had worked\n",
    "read_result = pd.read_csv(file_name, delimiter=\" \")\n",
    "read_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save weights and biases in a .txt file, so that if we want to plot the weights we don't have to run it all again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p \"Weights\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_weights:\n",
    "    dir_name = \"Weights\"\n",
    "    name = f\"Adam_{n}\" if ADAM else f\"Vanilla_{n}\"\n",
    "    name += \"_one_hot\" if one_hot else \"\"\n",
    "    name += \"_CenteringTrick\" if centering else \"\" \n",
    "    name += \"_Spin\" if SPINS else \"_NO-Spin\" \n",
    "    variables_or = [w,a,b]\n",
    "    variables = [\"w\",\"a\",\"b\"]\n",
    "    file_names = [dir_name+\"/\"+name+variable+\".dat\" for variable in variables]\n",
    "    for i in range(len(file_names)):\n",
    "        np.savetxt(file_names[i], variables_or[i], delimiter=\" \", comments=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f7215b3a7a598eb87d796f8da2e6ae65e9b6237dbe893c88938c519b77b4f916"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
